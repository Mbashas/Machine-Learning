{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salarMokhtariL/Fake-News-Detection-using-DistilBERT-Pretrained-Model-and-Transfer-Learning/blob/main/fake_news_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndc7_491r82o"
      },
      "source": [
        "# Fake News Detection\n",
        "> By Salar Mokhtari Laleh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxiA1oNssM2-"
      },
      "source": [
        "# Import Required Libraries\n",
        "\n",
        "Importing the necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs1IMiUVsZzO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D9GJ-oofr8ZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "from transformers import DistilBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcBVA15rscR3"
      },
      "source": [
        "# Load the Dataset\n",
        "\n",
        "Load the dataset into a pandas DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zxuSojQnsYLG"
      },
      "outputs": [],
      "source": [
        "# Load train data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Omulema abiisanyizza ne Maama bbebi mu kusala ...</td>\n",
              "      <td>Labayo omukyala ono ow'okugulu okumu bw'agatta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ab'oludda oluvuganya bataddemu ggiya mu kalulu</td>\n",
              "      <td>AKULIRA oludda oluwabula mu palamenti Mathias ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Omugagga yeetuze n'aleka ebibuuzo mu baffamire</td>\n",
              "      <td>Gonzanga Kalibbala 55, abadde musuubuzi wa mwe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bafulumizza lipooti ku basomesa</td>\n",
              "      <td>MMENGO esabye abasomesa obutalemera mu mirimu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Simon Peter Kasyate alondeddwa ku ky'omwogezi ...</td>\n",
              "      <td>Simon Peter Kasyate alondeddwa okubeera omwoge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11951</th>\n",
              "      <td>Bp. Luwalira avumiridde bannabbi ab'obulimba!</td>\n",
              "      <td>Luwalira yagambye nti bannabbi bangi abeeyita ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11952</th>\n",
              "      <td>Abakolera ku luguudo lw'eggaali y'omukka mu bi...</td>\n",
              "      <td>ABASUUBUZI abakolera ku luguudo lw'eggaali y'o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11953</th>\n",
              "      <td>Landiroodi alumye omupangisa ne yeewaana</td>\n",
              "      <td>OMUPANGISA atutte landiroodi ku poliisi lwa ku...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11954</th>\n",
              "      <td>Mmotoka ya Premio esaabadde ababadde bazina ak...</td>\n",
              "      <td>Akabenje kano kaguddewo ku ssaawa 3:00 ez'ekir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11955</th>\n",
              "      <td>Ebifaananyi  poliisi ng'ekwata ababaka ba pala...</td>\n",
              "      <td>Biibino ebifaananyi ebiraga poliisi engeri gye...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11956 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  \\\n",
              "0      Omulema abiisanyizza ne Maama bbebi mu kusala ...   \n",
              "1         Ab'oludda oluvuganya bataddemu ggiya mu kalulu   \n",
              "2         Omugagga yeetuze n'aleka ebibuuzo mu baffamire   \n",
              "3                        Bafulumizza lipooti ku basomesa   \n",
              "4      Simon Peter Kasyate alondeddwa ku ky'omwogezi ...   \n",
              "...                                                  ...   \n",
              "11951      Bp. Luwalira avumiridde bannabbi ab'obulimba!   \n",
              "11952  Abakolera ku luguudo lw'eggaali y'omukka mu bi...   \n",
              "11953           Landiroodi alumye omupangisa ne yeewaana   \n",
              "11954  Mmotoka ya Premio esaabadde ababadde bazina ak...   \n",
              "11955  Ebifaananyi  poliisi ng'ekwata ababaka ba pala...   \n",
              "\n",
              "                                                    text  \n",
              "0      Labayo omukyala ono ow'okugulu okumu bw'agatta...  \n",
              "1      AKULIRA oludda oluwabula mu palamenti Mathias ...  \n",
              "2      Gonzanga Kalibbala 55, abadde musuubuzi wa mwe...  \n",
              "3      MMENGO esabye abasomesa obutalemera mu mirimu ...  \n",
              "4      Simon Peter Kasyate alondeddwa okubeera omwoge...  \n",
              "...                                                  ...  \n",
              "11951  Luwalira yagambye nti bannabbi bangi abeeyita ...  \n",
              "11952  ABASUUBUZI abakolera ku luguudo lw'eggaali y'o...  \n",
              "11953  OMUPANGISA atutte landiroodi ku poliisi lwa ku...  \n",
              "11954  Akabenje kano kaguddewo ku ssaawa 3:00 ez'ekir...  \n",
              "11955  Biibino ebifaananyi ebiraga poliisi engeri gye...  \n",
              "\n",
              "[11956 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c0-eeQKO6HIC"
      },
      "outputs": [],
      "source": [
        "train_data.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Add a label column with 50-50 distribution of 0 and 1\n",
        "num_rows = len(train_data)\n",
        "labels = [0] * (num_rows // 2) + [1] * (num_rows - num_rows // 2)\n",
        "np.random.shuffle(labels)\n",
        "train_data['label'] = labels\n",
        "\n",
        "# Rename columns in train_data and test_data\n",
        "train_data.rename(columns={'article': 'text'}, inplace=True)\n",
        "test_data.rename(columns={'article': 'text'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bannakalungu muleete abaana tubageme Pooliyo k...</td>\n",
              "      <td>Bagumiziddwa nti okugema Pooliyo tekusoose bus...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Minisita Kasaija ayanjudde enteekateeka y'eggw...</td>\n",
              "      <td>MINISTA w'ebyensimbi Matia Kasaija ayanjude en...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Baziise 25 abatalinaako baabwe</td>\n",
              "      <td>ABANTU 25 be baziikiddwa wiiki ewedde e Bukasa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Obucaafu busattizza abasuubuzi e Bukolooto</td>\n",
              "      <td>ABATUUZE n'abasuubuzi abakolera mu tawuni y'e ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Owa Traffic eyabikiddwa nti afudde asangiddwa ...</td>\n",
              "      <td>OWA TULAFIKI eyakubiddwa  e Nakulabye  eyasoos...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11951</th>\n",
              "      <td>Ab'e Bukomero basattira lwa poliisi egenda oku...</td>\n",
              "      <td>Abatuuze bagamba nti akulira poliisi mu bitund...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11952</th>\n",
              "      <td>Abawangaalira ebweru w'eggwanga basabiddwa okw...</td>\n",
              "      <td>AKULIRA ekitongole  ekivunaanyizibwa okulondoo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11953</th>\n",
              "      <td>Anywedde omwenge n'agwa mu mazzi n'afa!</td>\n",
              "      <td>Abatuuze b'omu Sembule  Zooni e Kabowa ekisang...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11954</th>\n",
              "      <td>Kkooti ya munisipaali y'e Lubaga egenze ku tta...</td>\n",
              "      <td>KKOOTI ya munisipaali y'e Lubaga  ng'ekulembed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11955</th>\n",
              "      <td>DISITULIKITI Kadhi wa Mpigi Butambala ne Gomba...</td>\n",
              "      <td>DISITULIKITI Kadhi wa Mpigi Butambala ne Gomba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11860 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  \\\n",
              "0      Bannakalungu muleete abaana tubageme Pooliyo k...   \n",
              "1      Minisita Kasaija ayanjudde enteekateeka y'eggw...   \n",
              "2                         Baziise 25 abatalinaako baabwe   \n",
              "3             Obucaafu busattizza abasuubuzi e Bukolooto   \n",
              "4      Owa Traffic eyabikiddwa nti afudde asangiddwa ...   \n",
              "...                                                  ...   \n",
              "11951  Ab'e Bukomero basattira lwa poliisi egenda oku...   \n",
              "11952  Abawangaalira ebweru w'eggwanga basabiddwa okw...   \n",
              "11953            Anywedde omwenge n'agwa mu mazzi n'afa!   \n",
              "11954  Kkooti ya munisipaali y'e Lubaga egenze ku tta...   \n",
              "11955  DISITULIKITI Kadhi wa Mpigi Butambala ne Gomba...   \n",
              "\n",
              "                                                    text  label  \n",
              "0      Bagumiziddwa nti okugema Pooliyo tekusoose bus...      0  \n",
              "1      MINISTA w'ebyensimbi Matia Kasaija ayanjude en...      1  \n",
              "2      ABANTU 25 be baziikiddwa wiiki ewedde e Bukasa...      0  \n",
              "3      ABATUUZE n'abasuubuzi abakolera mu tawuni y'e ...      1  \n",
              "4      OWA TULAFIKI eyakubiddwa  e Nakulabye  eyasoos...      1  \n",
              "...                                                  ...    ...  \n",
              "11951  Abatuuze bagamba nti akulira poliisi mu bitund...      0  \n",
              "11952  AKULIRA ekitongole  ekivunaanyizibwa okulondoo...      0  \n",
              "11953  Abatuuze b'omu Sembule  Zooni e Kabowa ekisang...      1  \n",
              "11954  KKOOTI ya munisipaali y'e Lubaga  ng'ekulembed...      1  \n",
              "11955  DISITULIKITI Kadhi wa Mpigi Butambala ne Gomba...      1  \n",
              "\n",
              "[11860 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9h859kos6k1"
      },
      "source": [
        "# Prepare the Data\n",
        "Prepare the data for the PyTorch model. First, let's define a custom dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9-4ULm_Ps1f_"
      },
      "outputs": [],
      "source": [
        "''' This class takes in the data, tokenizes it using the DistilBertTokenizer from the transformers library,\n",
        " and returns the input IDs, attention masks, and labels.'''\n",
        "\n",
        "\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, data, max_len=128):\n",
        "        self.data = data\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.iloc[index]['text']\n",
        "        label = self.data.iloc[index]['label']\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0), torch.tensor(label, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D3t0jbkdtV02"
      },
      "outputs": [],
      "source": [
        "# split the data into training and validation sets\n",
        "\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2,\n",
        "                                        random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qoRnpQ_4tYr6"
      },
      "outputs": [],
      "source": [
        "# Create PyTorch data loaders for the training, validation, and test sets:\n",
        "\n",
        "train_dataset = FakeNewsDataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = FakeNewsDataset(val_data)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_dataset = FakeNewsDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6336</th>\n",
              "      <td>Ekitongole ekivunaanyizibwa ku kugula n'okutun...</td>\n",
              "      <td>Amagezi gano gaabaweereddwa Benson Turamye aku...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5938</th>\n",
              "      <td>Abajaasi ba UPDF 2  basimattuse  okufiira mu k...</td>\n",
              "      <td>Abajaasi ababadde mu mmotoka eno bwe balabye n...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4003</th>\n",
              "      <td>Batongozza okugaba ebyapa e Busoga, 700 babifunye</td>\n",
              "      <td>GAVUMENTI ng' eyita mu kitongole ky'ebyettaka ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3904</th>\n",
              "      <td>Gavumenti yeddizza University ya Busoga lwa bb...</td>\n",
              "      <td>ABAKULEMBEZE b'ekkanisa y'Obukuristaayo bawadd...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>Abantu mukomye okusoomooza abeebyokwerinda</td>\n",
              "      <td>Ven. Rev Canon Godffrey BK Buwembo, Ssaabadink...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11670</th>\n",
              "      <td>Ebizuuse ku muserikale eyasindidde muganzi we ...</td>\n",
              "      <td>Oluk abadde ddereeva w'omuduumizi wa poliisi w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9590</th>\n",
              "      <td>Nanziri alinnyisizza ggiya eneekuba Munnakenya</td>\n",
              "      <td>CATHERINE Nanziri yeesomye okusitukira mu musi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4791</th>\n",
              "      <td>Gavt. eraze amasomero ag'okuzimba mu bajeti</td>\n",
              "      <td>GAVUMENTI eraze amasomero ga Pulayimale ne Sin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1834</th>\n",
              "      <td>Akulira eddwaaliro ly'e Mbarara yeeraliikiridd...</td>\n",
              "      <td>Akulira eddwaaliro lya Mbarara bw'abadde ayoge...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Ssabakristu w'ekigo kya Mt. Camel e Busega afu...</td>\n",
              "      <td>Alex Kyagamba munnamawulire w'ekigo kino atege...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6072 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  \\\n",
              "6336   Ekitongole ekivunaanyizibwa ku kugula n'okutun...   \n",
              "5938   Abajaasi ba UPDF 2  basimattuse  okufiira mu k...   \n",
              "4003   Batongozza okugaba ebyapa e Busoga, 700 babifunye   \n",
              "3904   Gavumenti yeddizza University ya Busoga lwa bb...   \n",
              "2001          Abantu mukomye okusoomooza abeebyokwerinda   \n",
              "...                                                  ...   \n",
              "11670  Ebizuuse ku muserikale eyasindidde muganzi we ...   \n",
              "9590      Nanziri alinnyisizza ggiya eneekuba Munnakenya   \n",
              "4791         Gavt. eraze amasomero ag'okuzimba mu bajeti   \n",
              "1834   Akulira eddwaaliro ly'e Mbarara yeeraliikiridd...   \n",
              "498    Ssabakristu w'ekigo kya Mt. Camel e Busega afu...   \n",
              "\n",
              "                                                    text  label  \n",
              "6336   Amagezi gano gaabaweereddwa Benson Turamye aku...      1  \n",
              "5938   Abajaasi ababadde mu mmotoka eno bwe balabye n...      1  \n",
              "4003   GAVUMENTI ng' eyita mu kitongole ky'ebyettaka ...      1  \n",
              "3904   ABAKULEMBEZE b'ekkanisa y'Obukuristaayo bawadd...      1  \n",
              "2001   Ven. Rev Canon Godffrey BK Buwembo, Ssaabadink...      0  \n",
              "...                                                  ...    ...  \n",
              "11670  Oluk abadde ddereeva w'omuduumizi wa poliisi w...      1  \n",
              "9590   CATHERINE Nanziri yeesomye okusitukira mu musi...      1  \n",
              "4791   GAVUMENTI eraze amasomero ga Pulayimale ne Sin...      0  \n",
              "1834   Akulira eddwaaliro lya Mbarara bw'abadde ayoge...      1  \n",
              "498    Alex Kyagamba munnamawulire w'ekigo kino atege...      0  \n",
              "\n",
              "[6072 rows x 3 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXoyz5NiuR7x"
      },
      "source": [
        "# Define the Model\n",
        "define the PyTorch model. We'll use the `DistilBertForSequenceClassification` model from the `transformers` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e5T1YcxauKpe"
      },
      "outputs": [],
      "source": [
        "class FakeNewsClassifier(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(FakeNewsClassifier, self).__init__()\n",
        "        self.bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5opyKlLujgq"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "With the data and model prepared, we can now train the model using PyTorch. We'll define a function to train the model for one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MJT_-swAujJn"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, criterion, train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    for input_ids, attention_mask, labels in tqdm(train_loader, desc='Training'):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (outputs.argmax(1) == labels.to(device)).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc /= len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRTL30JYusZy"
      },
      "source": [
        "This function takes in the model, optimizer, loss function, and data loader, and performs a forward pass through the model, calculates the loss, and performs backpropagation and gradient descent to update the model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXAZkgv8utqw"
      },
      "source": [
        "We'll also define a function to evaluate the model on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Fz5hGObLucd8"
      },
      "outputs": [],
      "source": [
        "def eval_epoch(model, criterion, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in tqdm(val_loader, desc='Validation'):\n",
        "            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (outputs.argmax(1) == labels.to(device)).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc /= len(val_loader.dataset)\n",
        "\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfYQ5ey_uwhZ"
      },
      "source": [
        "This function takes in the model, loss function, and data loader, and performs a forward pass through the model to calculate the loss and accuracy on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gaq98MZWuyDE"
      },
      "source": [
        "Now we can define the main training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using:**cuda**\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using:**{device}**\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available? True\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.8\n"
          ]
        }
      ],
      "source": [
        "import torch.version\n",
        "\n",
        "\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Model: Quadro P1000\n"
          ]
        }
      ],
      "source": [
        "print(\"GPU Model:\",torch.cuda.get_device_name(0) if torch.cuda.is_available else \"None\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZQCu1YIQuvTm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Training: 100%|██████████| 190/190 [10:50<00:00,  3.42s/it]\n",
            "Validation: 100%|██████████| 48/48 [01:00<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss=0.6951, Train Acc=0.5021, Val Loss=0.6950, Val Acc=0.4776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 190/190 [10:32<00:00,  3.33s/it]\n",
            "Validation: 100%|██████████| 48/48 [00:59<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss=0.6941, Train Acc=0.4942, Val Loss=0.6928, Val Acc=0.5204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 190/190 [10:29<00:00,  3.32s/it]\n",
            "Validation: 100%|██████████| 48/48 [00:59<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss=0.6941, Train Acc=0.5056, Val Loss=0.6942, Val Acc=0.4802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 190/190 [10:27<00:00,  3.31s/it]\n",
            "Validation: 100%|██████████| 48/48 [01:00<00:00,  1.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Train Loss=0.6916, Train Acc=0.5267, Val Loss=0.6952, Val Acc=0.5105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 190/190 [10:26<00:00,  3.30s/it]\n",
            "Validation: 100%|██████████| 48/48 [00:59<00:00,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Train Loss=0.6809, Train Acc=0.5660, Val Loss=0.7164, Val Acc=0.5013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = FakeNewsClassifier().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader)\n",
        "    val_loss, val_acc = eval_epoch(model, criterion, val_loader)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        best_val_acc = val_acc"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEmBbcU5DFcDF753QI21ez",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
